{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¦„ðŸ¦„ðŸ¦„       ðŸ¦„ðŸ¦„ðŸ¦„     ðŸ¦„ðŸ¦„ðŸ¦„      ðŸ¦„ðŸ¦„ðŸ¦„      ðŸ¦„ðŸ¦„ðŸ¦„      ðŸ¦„ðŸ¦„ðŸ¦„     ðŸ¦„ðŸ¦„ðŸ¦„    ðŸ¦„ðŸ¦„ðŸ¦„    ðŸ¦„ðŸ¦„ðŸ¦„   ðŸ¦„ðŸ¦„ðŸ¦„    ðŸ¦„ðŸ¦„ðŸ¦„     ðŸ¦„ðŸ¦„ðŸ¦„ ðŸ¦„ðŸ¦„ðŸ¦„ ðŸ¦„ðŸ¦„ðŸ¦„ ðŸ¦„ðŸ¦„ðŸ¦„ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the ETL task for getting your UnicornðŸ¦„ Rental Bussiness to New heights. \n",
    "\n",
    "In this notebook, we will carry out following ETL Tasks:\n",
    "\n",
    "1. Download Data: Get the Data from prepopulated Aurora Database from your account.  \n",
    "2. Data Cleaning: Clean the data, add/delete/rename columns in the data downloaded in step 1.\n",
    "3. Data Upload: Upload these cleaned csv's to S3 bucket. \n",
    "\n",
    "As a player, you will need to fill in values or modify the code wherever it is noted as `PlayerInputRequired` \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Downloadig Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this task, we will: \n",
    "    1.1. Install Dependencies\n",
    "    1.2. Connect to database \n",
    "    1.3. Query database tables for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Install Dependencies\n",
    "\n",
    "psycopg2 is a python package to set up connection with rds database which holds the data. As psycopg2 has multiple depencies which may conflict with other packages when installed using pip, we will installing this package using conda install. Conda install handles dependcies better given the numerous conda environments deployed by sagemaker studio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Connect to Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up connection with database, we will need following database credentials. From your AWS console, look up hostname(endpoint), region and password. We will initialize all the credentials/params in the following cell. \n",
    "Following is an example: \n",
    " \n",
    "```\n",
    "host=\"gameday-aurora-postgres.cluster-cj8pr0iv1jkb.us-east-2.rds.amazonaws.com\"     <- db endpoint  (fetch it from aws console)\n",
    "PORT=\"5432\"                                                                         <- default\n",
    "USER=\"postgres\"                                                                     <- default\n",
    "DBNAME=\"postgres\"                                                                   <- DBName \n",
    "password = \"fn3t8ZgN06ABpSrT\"                                                       <- database password  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> In following cell, find and replace values for db endpoint(host) and db password. db endpoint can be found on AWS rds console and db password is saved in Secret Manager. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "host=\"ENDPOINT_PlayerInputRequired\"                         # PlayerInputRequired                           \n",
    "PORT=\"5432\"\n",
    "USER=\"postgres\"\n",
    "DBNAME=\"postgres\"\n",
    "password = \"dbPassword_PlayerInputRequired\"                 # PlayerInputRequired "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Destination bucket to upload cleaned data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_s3Bucket = 'S3_BUCKET_NAME_PlayerInputRequired'   # PlayerInputRequired Look up your deployed stack and replace bucket name with yours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import python packages and set up connection using connect method of psycopg2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "conn = psycopg2.connect(host = host, \n",
    "                        database = DBNAME, \n",
    "                        user = USER, \n",
    "                        password = password)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Query database tables for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modify following queries to select desired columns from database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify query to select columns productkey, productsubcategorykey, listprice from `product` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df=pd.read_sql(\"\"\"SELECT *  FROM gameday.product\"\"\", conn)     # PlayerInputRequired "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify query to select columns customerkey, gender,yearlyincome from `customer` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df=pd.read_sql(\"\"\"SELECT * FROM gameday.customer \"\"\", conn)    # PlayerInputRequired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify query to select productkey, customerkey, orderdate from `internetsales` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df=pd.read_sql(\"\"\"SELECT * FROM gameday.internetsales\"\"\", conn)  # PlayerInputRequired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the original interactions as it will be referenced couple of time after cleaning\n",
    "interactions_df_orig = interactions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean `interactions_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# filter unique product keys\n",
    "unique_product_keys = set(items_df['productkey'].unique())  \n",
    "\n",
    "# filter interactions data rows where product key exists in unique product key\n",
    "interactions_df = interactions_df[interactions_df['productkey'].isin(unique_product_keys)]  \n",
    "\n",
    "#convert order datetime column datatype to pandas datetime data type\n",
    "interactions_df['OrderDate_datetime'] = pd.to_datetime(interactions_df['orderdate'])\n",
    "\n",
    "#add column TIMESTAMP which contains above OrderDate time converted in numpy int64 datatype\n",
    "interactions_df['TIMESTAMP'] = interactions_df.OrderDate_datetime.values.astype(np.int64) // 10 ** 9\n",
    " \n",
    "# column rename column 'productkey' to 'ITEM_ID' and column 'customerkey' to 'USER_ID'\n",
    "interactions_df = interactions_df.rename(columns={'productkey':'ITEM_ID', 'customerkey':'USER_ID'}) \n",
    "\n",
    "# select columns ITEM_ID, USER_ID and TIMESTAMP only from interactions dataframe and drop other columns. \n",
    "interactions_df = interactions_df[['ITEM_ID', 'USER_ID', 'TIMESTAMP']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets print how interactions_df dataframe looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save cleaned df to csv in local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os and pathlib to create directory locally\n",
    "import os\n",
    "from pathlib import Path \n",
    "\n",
    "# path where we want to store the cleaned interactions dataframe\n",
    "path = Path(\"./data/clean\")\n",
    "\n",
    "# create the directory structure if it doesnt exist\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# save the dataframe to a csv file named interactions.csv\n",
    "interactions_df.to_csv(os.path.join(path, 'interactions.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean `users_df` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter unique product keys\n",
    "unique_user_keys = set(interactions_df_orig['customerkey'].unique())\n",
    "\n",
    "# filter users data rows where product key exists in unique product key\n",
    "users_df = users_df[users_df['customerkey'].isin(unique_user_keys)]\n",
    "\n",
    "# column rename column customerkey to USER_ID, 'gender' to 'Gender', 'yearlyincome' to 'YearlyIncome\n",
    "users_df = users_df.rename(columns={'customerkey':'USER_ID', 'gender': 'Gender', 'yearlyincome': 'YearlyIncome'})\n",
    "\n",
    "# save users dataframe locally to users.csv file\n",
    "users_df.to_csv(os.path.join(path, 'users.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets print users_df dataframe to checkout cleaned version \n",
    "users_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean `items_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: there should be no mention of bikes. these values should come from the subcategory table. they are already transformed by Jose. Please consult with him about where to get them\n",
    "items_df = items_df[items_df['productsubcategorykey'] < 4]\n",
    "items_df['productsubcategorykey'] = items_df['productsubcategorykey'].replace({1: 'Mountain Cosmic Unicorn', 2: 'Road Cosmic Unicorn', 3: 'Touring Cosmic Unicorn'})\n",
    "items_df = items_df.rename(columns={'productkey':'ITEM_ID', 'productsubcategorykey':'ProductSubcategory', 'listprice': 'ListPrice'})\n",
    "\n",
    "items_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df.to_csv('./data/clean/items.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Upload data to S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous steps, we cleaned the data and saved dataframe into a csv on our local disk. To use this data for training in personalize, lets save it into s3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload to s3 bucket\n",
    "import boto3\n",
    "boto3.Session().resource('s3').Bucket(dest_s3Bucket)\\\n",
    "        .Object('data/interactions.csv').upload_file('./data/clean/interactions.csv')\n",
    "\n",
    "#upload to s3 bucket\n",
    "import boto3\n",
    "boto3.Session().resource('s3').Bucket(dest_s3Bucket)\\\n",
    "        .Object('data/items.csv').upload_file('./data/clean/items.csv')\n",
    "\n",
    "#upload to s3 bucket\n",
    "import boto3\n",
    "boto3.Session().resource('s3').Bucket(dest_s3Bucket)\\\n",
    "        .Object('data/users.csv').upload_file('./data/clean/users.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the cleaned csv files are on S3, we will be importing these into personlize engine for training in next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
